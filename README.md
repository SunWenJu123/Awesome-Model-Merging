# Awesome-Model-Merging

Paper collection, Summary, Code for Model Merging

## Paper

### Model Merging

+   [Arxiv2022] Fusing finetuned models for better pretraining ([paper](https://arxiv.org/pdf/2204.03044))
+   [Arxiv2022] Branch-Train-Merge: Embarrassingly Parallel Training of Expert Language Models ([paper](https://arxiv.org/pdf/2208.03306))([code](https://github.com/hadasah/btm))

+   [NIPS2022] Merging Models with Fisher-Weighted Averaging ([paper](https://proceedings.neurips.cc/paper_files/paper/2022/hash/70c26937fbf3d4600b69a129031b66ec-Abstract-Conference.html))([code](https://github.com/mmatena/model_merging))
+   [ICLR2023] Git Re-Basin: Merging Models modulo Permutation Symmetries ([paper](https://openreview.net/pdf?id=CQsmMYmlP5T))([code](https://github.com/samuela/git-re-basin))
+   [ICLR2023] Editing models with task arithmetic ([paper](https://openreview.net/pdf?id=6t0Kwf8-jrj)) ([code](https://github.com/mlfoundations/task_vectors))
+   [NIPS2023] Ties-Merging: Resolving Interference When Merging Models ([paper](https://proceedings.neurips.cc/paper_files/paper/2023/file/1644c9af28ab7916874f6fd6228a9bcf-Paper-Conference.pdf))([code](https://github.com/prateeky2806/ties-merging))

+   [ICLR2024] AdaMerging: Adaptive Model Merging for Multi-Task Learning ([paper](https://openreview.net/pdf?id=nZP6NgD3QY))([code](https://github.com/EnnengYang/AdaMerging))
+   [ICLR2024] Mixture of LoRA Experts ([paper](https://openreview.net/pdf?id=uWvKBCYh4S))([code](https://github.com/yushuiwx/MoLE))
+   