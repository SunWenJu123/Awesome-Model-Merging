# Awesome-Model-Merging

Paper collection, Summary, Code for Model Merging

## Survey

[Arxiv2023] Deep Model Fusion: A Survey ([paper](https://arxiv.org/pdf/2309.15698))

## Paper

### for Multi-task Model Merging

+   [ICLR2024] AdaMerging: Adaptive Model Merging for Multi-Task Learning ([paper](https://openreview.net/pdf?id=nZP6NgD3QY))([code](https://github.com/EnnengYang/AdaMerging))
+   [ICLR2024] Mixture of LoRA Experts ([paper](https://openreview.net/pdf?id=uWvKBCYh4S))([code](https://github.com/yushuiwx/MoLE))
+   [ICLR2024] Model Merging by Uncertainty-Based Gradient Matching ([paper](https://openreview.net/pdf?id=D7KJmfEDQP))([code](https://github.com/UKPLab/iclr2024-model-merging))
+   [ICLR2024] Knowledge Fusion of Large Language Models ([paper](https://openreview.net/pdf?id=jiDsk12qcz))([code](https://github.com/fanqiwan/FuseAI))
+   [ICLR2024] ZipIt! Merging Models from Different Tasks without Training ([paper](https://openreview.net/pdf?id=LEYUkvdUhq))([code](https://github.com/gstoica27/ZipIt))
+   [ICLR2023] Git Re-Basin: Merging Models modulo Permutation Symmetries ([paper](https://openreview.net/pdf?id=CQsmMYmlP5T))([code](https://github.com/samuela/git-re-basin))
+   [ICLR2023] Editing models with task arithmetic ([paper](https://openreview.net/pdf?id=6t0Kwf8-jrj)) ([code](https://github.com/mlfoundations/task_vectors))
+   [ICLR2023] Dataless Knowledge Fusion by Merging Weights of Language Models ([paper](https://openreview.net/pdf?id=FCnohuR6AnM))([code](https://github.com/bloomberg/dataless-model-merging))
+   [NeurIPS2023] Ties-Merging: Resolving Interference When Merging Models ([paper](https://proceedings.neurips.cc/paper_files/paper/2023/file/1644c9af28ab7916874f6fd6228a9bcf-Paper-Conference.pdf))([code](https://github.com/prateeky2806/ties-merging))
+   [NeurIPS2023] Task Arithmetic in the Tangent Space: Improved Editing of Pre-Trained Models ([paper](https://proceedings.neurips.cc/paper_files/paper/2023/file/d28077e5ff52034cd35b4aa15320caea-Paper-Conference.pdf))([code](https://github.com/gortizji/tangent_task_arithmetic))
+   [Arxiv2022] Fusing finetuned models for better pretraining ([paper](https://arxiv.org/pdf/2204.03044))
+   [Arxiv2022] Branch-Train-Merge: Embarrassingly Parallel Training of Expert Language Models ([paper](https://arxiv.org/pdf/2208.03306))([code](https://github.com/hadasah/btm))

+   [NIPS2022] Merging Models with Fisher-Weighted Averaging ([paper](https://proceedings.neurips.cc/paper_files/paper/2022/hash/70c26937fbf3d4600b69a129031b66ec-Abstract-Conference.html))([code](https://github.com/mmatena/model_merging))

### for Single-task Generalization

+   [ICLR2024] Sparse Model Soups: A Recipe for Improved Pruning via Model Averaging ([paper](https://openreview.net/pdf?id=xx0ITyHp3u))([code](https://github.com/ZIB-IOL/SMS))
+   [ICLR2024] Sparse Weight Averaging With Multiple Participles For Iterative Magnitude Pruning ([paper](https://openreview.net/notes/edits/attachment?id=NTvAsil2kk&name=pdf))

+   [ICLR2023] Trainable Weight Averaging: Efficient Training by Optimizing Historical Solutions ([paper](https://openreview.net/pdf?id=8wbnpOJY-f))([code](https://github.com/nblt/TWA))
+   [CVPR2023] Seasoning Model Soups for Robustness to Adversarial and Natural Distribution Shifts ([paper](https://openaccess.thecvf.com/content/CVPR2023/papers/Croce_Seasoning_Model_Soups_for_Robustness_to_Adversarial_and_Natural_Distribution_CVPR_2023_paper.pdf))
+   [ICML2023] Instant Soup: Cheap Pruning Ensembles in A Single Pass Can Draw Lottery Tickets from Large Models ([paper](https://proceedings.mlr.press/v202/jaiswal23b/jaiswal23b.pdf))([code](https://github.com/VITA-Group/instant_soup))
+   [AAAI2023] Lottery Pools: Winning More by Interpolating Tickets without Increasing Training or Inference Cost ([paper](file:///C:/Users/SunWenju/Downloads/26297-Article%20Text-30360-1-2-20230626.pdf))([code](https://github.com/luuyin/Lottery-pools))
+   [CVPR2022] Robust fine-tuning of zero-shot models ([paper](https://openaccess.thecvf.com/content/CVPR2022/papers/Wortsman_Robust_Fine-Tuning_of_Zero-Shot_Models_CVPR_2022_paper.pdf))
+   [ICML2022] Model soups: averaging weights of multiple fine-tuned models improves accuracy without increasing inference time ([paper](https://proceedings.mlr.press/v162/wortsman22a/wortsman22a.pdf))([code](https://github.com/mlfoundations/model-soups))
+   [ICML2022] Superposing many tickets into one: A performance booster for sparse neural network training ([paper](https://proceedings.mlr.press/v180/yin22a/yin22a.pdf))
+   [NeurIPS2022] Diverse Weight Averaging for Out-of-Distribution Generalization ([paper](https://proceedings.neurips.cc/paper_files/paper/2022/file/46108d807b50ad4144eb353b5d0e8851-Paper-Conference.pdf))([code](https://github.com/alexrame/diwa))
+   [NeurIPS2022] Ensemble of Averages: Improving Model Selection and Boosting Performance in Domain Generalization ([paper](https://proceedings.neurips.cc/paper_files/paper/2022/file/372cb7805eaccb2b7eed641271a30eec-Paper-Conference.pdf))([code](https://github.com/salesforce/ensemble-of-averages))
+   [NeurIPS2021] SWAD: Domain Generalization by Seeking Flat Minima ([paper](https://proceedings.neurips.cc/paper_files/paper/2021/file/bcb41ccdc4363c6848a1d760f26c28a0-Paper.pdf))([code](https://github.com/khanrc/swad))
+   [UAI2018] Averaging weights leads to wider optima and better generalization ([paper](https://auai.org/uai2018/proceedings/papers/313.pdf))([code](https://github.com/timgaripov/swa))

+   [SIAM1992] Acceleration of Stochastic Approximation by Averaging ([paper](https://www.researchgate.net/profile/Boris-Polyak-2/publication/236736831_Acceleration_of_Stochastic_Approximation_by_Averaging/links/0f31753227e964baab000000/Acceleration-of-Stochastic-Approximation-by-Averaging.pdf))

### for Continual Learning

+   [Arxiv2023] Towards Plastic and Stable Exemplar-Free Incremental Learning: A Dual-Learner Framework with Cumulative Parameter Averaging ([paper](https://arxiv.org/abs/2310.18639))
+   [ICCV2023] Tangent Model Composition for Ensembling and Continual Fine-tuning ([paper](https://openaccess.thecvf.com/content/ICCV2023/papers/Liu_Tangent_Model_Composition_for_Ensembling_and_Continual_Fine-tuning_ICCV_2023_paper.pdf))([code](https://github.com/tianyu139/))
+   [ICML2023] Optimizing Mode Connectivity for Class Incremental Learning ([paper](https://proceedings.mlr.press/v202/wen23b/wen23b.pdf))([code](https://github.com/HaitaoWen/EOPC))
+   [ICLR2021] Linear Mode Connectivity in Multitask and Continual Learning ([paper](https://openreview.net/pdf?id=Fmg_fQYUejf))([code](https://github.com/imirzadeh/MC-SGD))
+   [NeurIPS2017] Overcoming Catastrophic Forgetting by Incremental Moment Matching ([paper](https://proceedings.neurips.cc/paper_files/paper/2017/file/f708f064faaf32a43e4d3c784e6af9ea-Paper.pdf))([code](https://github.com/btjhjeon/IMM_tensorflow))

## Videos

+   [Merge LLMs to Make Best Performing AI Model](https://www.youtube.com/watch?v=byf-y0P4hMg)
+   [Paper deep dive: Evolutionary Optimization of Model Merging Recipes](https://www.youtube.com/watch?v=BihyfzOidDI)
+   [https://www.youtube.com/watch?v=cvOpX75Kz4M](https://www.youtube.com/watch?v=cvOpX75Kz4M)

## Other Resources

Other **Awesome** lists that I thought may share techniques for Model Merging.

+   [Awesome Incremental Learning / Lifelong learning](https://github.com/xialeiliu/Awesome-Incremental-Learning)
+   [Awesome Deep Neural Network Compression](https://github.com/csyhhu/Awesome-Deep-Neural-Network-Compression?tab=readme-ov-file)

